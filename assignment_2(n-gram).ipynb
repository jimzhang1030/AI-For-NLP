{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the main points of this lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. How to Github and Why do we use Jupyter and Pycharm;***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* git init; git add; git commit -m; git remote add (name) (repository); git push -u (name)\n",
    "* Because Jupyter and Pycharm is Integrated Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. What's the Probability Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "$$ Pr(s) = Pr(w_1\\cdot w_2\\cdots w_n) \\sim Pr(w_1)\\cdot Pr(w_2|w_1)\\cdots Pr(w_n|w_{n-1}\\cdot w_{n-2}\\cdots w_1 )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Can you came up with some sceneraies at which we could use Probability Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* Speech recognition\n",
    "* OCR\n",
    "* Text understanding\n",
    "* Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* Perfrom well\n",
    "* It' hard to realize and it needs lost of pattern, whta's more, it's hard to cover all pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5. what's the Language Mode?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* It is used to calculate the probability of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6. Can you came up with some sceneraies at which we could use Language Model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* Speech recognition\n",
    "* Text understanding\n",
    "* Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***7. What's the 1-gram language model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "$$ Pr(s) = Pr(w_1\\cdot w_2\\cdots w_n)\\sim Pr(w_1)\\cdot Pr(w_2)\\cdots Pr(w_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***8. What's the disadvantages and advantages of 1-gram language model?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* disadvantages: No constraints for the next word!\n",
    "* advantages: Compared to the caculate the probability model,it's convenient to compute.It simplifies the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***9. What't the 2-gram models？***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "$$ Pr(s) = Pr(w_1\\cdot w_2\\cdots w_n)\\sim Pr(w_1)\\cdot Pr(w_2|w_1)\\cdot Pr(w_3|w_2)\\cdots Pr(w_n|w_{n-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***10. what's the web crawler, and can you implement a simple crawler?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* web crawler is crawl the information you need from web\n",
    "* Yes, I can use some python package to do it, such as request to get url, and regular expression to match the information you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***11. There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "  difficults:\n",
    "* it will constraint the number of the same ip access in a period\n",
    "* verification code  \n",
    "\n",
    "solution:\n",
    "* ip address warehouse\n",
    "* buy some service for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***12. What't the Regular Expression and how to use?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* It' s used for string matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using wikipedia dataset to finish the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hanziconv\n",
      "  Downloading https://files.pythonhosted.org/packages/63/71/b89cb63077fd807fe31cf7c016a06e7e579a289d8a37aa24a30282d02dd2/hanziconv-0.3.2.tar.gz (276kB)\n",
      "Building wheels for collected packages: hanziconv\n",
      "  Building wheel for hanziconv (setup.py): started\n",
      "  Building wheel for hanziconv (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\zzy\\AppData\\Local\\pip\\Cache\\wheels\\03\\d8\\3c\\c39898fa9c9ce6e34b0ab4c6604892462d440c743715c94054\n",
      "Successfully built hanziconv\n",
      "Installing collected packages: hanziconv\n",
      "Successfully installed hanziconv-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hanziconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = re.compile(\"<{1}[^<>]*>{1}\")         #匹配xml标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因机器运行时间原因，1.2G的训练文件我只取了100M进行训练\n",
    "input_file = open('C:/Users/zzy/wikiextractor-master/zhwiki_per100M/AA/wiki_00','r',encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('./result_00', 'w+', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_contents = \"\"\n",
    "for line in input_file.readlines():\n",
    "    line = line.strip('\\n')\n",
    "#     line = jieba.lcut(HanziConv.toSimplified(token(p2.sub('', text_str))))\n",
    "    line = token(p2.sub('', line))                            #去除xml标签，并去除标点符号\n",
    "    line = HanziConv.toSimplified(line)                       #化繁体为简体\n",
    "    words = jieba.lcut(line)                                  #中文分词\n",
    "    words = [t for t in words if t.strip() and t != 'n']      #去空格和换行符留下的‘n'\n",
    "    for word in words:\n",
    "        article_contents += word + \" \"\n",
    "#     output.write(article_contents)\n",
    "# output.close()\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49732547\n"
     ]
    }
   ],
   "source": [
    "print(len(article_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'数学 数学 是 利用 符号语言 研究 数量 结构 变化 以及 空间 等 概念 的 一门 学科 从 某种 角度看 属于 形式 科学 的 一种 数学 透过 抽象化 和 逻辑推理 的 使用 由 计数 计算 '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_contents[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens = article_contents.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17111317"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the frequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences = [f for w, f in words_count.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences_sum = sum(frequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(word):\n",
    "    esp = 1 / frequences_sum\n",
    "    if word in words_count:\n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prodect(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = jieba.lcut(string)\n",
    "    return product([get(prob(t)) for t in words]) # 注意加上[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_grams_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter:\n",
    "        return _2_gram_counter[w1 + w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_2_gram(w1, w2):\n",
    "        return get_combination_prob(w1, w2) / get_prob(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_of_2_gram(sentence):\n",
    "    sentence_probility = 1\n",
    "    words = jieba.lcut(sentence)\n",
    "    for i, word in enumerate(words):\n",
    "        if i==0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i - 1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "            \n",
    "        sentence_probility *= prob\n",
    "        \n",
    "    return sentence_probility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\n",
    "'我今天吃火锅 火锅今天吃我',\n",
    "'真事一只好看的小猫 真是一只好看的小猫'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真是一只好看的小猫 is more possible\n",
      "我今天吃火锅 is more possible\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_of_2_gram(s1), language_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    print('{} is more possible'.format(better))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: If we need to solve following problems, how can language model help us?\n",
    "\n",
    "* Voice Recognization.\n",
    "\n",
    "* Sogou pinyin input.\n",
    "* Auto correction in search engine.\n",
    "* Abnormal Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "* advantage:\n",
    ">1. easy to code\n",
    ">2. it can be applied to lots of problems\n",
    ">3. don' need to write losts of pattern, it is data driven\n",
    "* disadvantage:\n",
    ">*"
   ]
  },
  {
=======
>>>>>>> 6defce347fe8f615512e227b3f37a30aa59e3e5b
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
